# Lecture 2 Making Good Decisions Given a Model of the World

# 课时2 已知环境模型时，如何做出好的决策 2019.01.09

## 3. 在马尔可夫决策过程中做出动作（Acting in a Markov decision process）

我们从回忆模型（model）、策略（policy）和值函数（value function）的定义开始。令行为体的状态空间和动作空间分别表示为 $S$ 和 $A$，那么，

$\bullet$ 模型：模型是行为体所处的环境的状态转移与奖励的数学模型，包括状态转移概率 P(s'|s,a)，表示状态 $s\in S$ 在执行动作 $a\in A$ 后，状态转移到 $s'\in S$ 后的概率，以及在状态 $s\in S$ 执行动作 $a\in A$ 后得到的奖励 $R(s,a)$（确定的或随机的）。

$\bullet$ 策略：策略是一个将行为体的状态映射到动作的函数 $\pi:S\rightarrow A$。

$\bullet$ 值函数：对应于特定策略 $\pi$ 和状态 $s\in S$ 的值函数 $V^{\pi}$ 是行为体从状态 $s$ 开始，遵循策略 $\pi$ 所能获得的未来（衰减）奖励的累加。

同时回忆一下上节课提到的马尔可夫性质（Markov property）的概念。考虑一个遵循某个转移规律的随机过程 $(s_0,s_1,s_2,...)$，我们称这个随机过程有马尔可夫性质当且仅当对于 $\forall i=1,2,...$，$P(s_i|s_0,...,s_{i-1})=P(s_i|s_{i-1})$，即以包括当前状态在内的历史为条件的转移到下一状态的概率，与仅以当前状态为条件的转移到下一状态的概率相等。在这种情况下，当前状态是随机过程的历史的一个充分统计，我们认为“未来与过去无关”。

本课中，我们将在这些定义的基础上，首先定义马尔可夫过程（Markov process，MP），然后定义马尔可夫回报过程（Markov reward process，MRP），最后定义马尔可夫决策过程（Markov decision process，MDP）。我们将通过讨论一些算法来结束本课，这些算法使我们能够在 MDP 完全已知的情况下做出好的决策。

### 3.1 马尔可夫过程（Markov Process）

在最一般的情况下，马尔可夫过程是一个满足马尔可夫性质的随机过程，因此我们称马尔可夫过程是“无记忆的”。在本课中，我们将提出两个在强化学习设定中非常常见的假设：

$\bullet$ 有限的状态空间（finite state space）：马尔可夫过程的状态空间是有限的。这意味着对于马尔可夫过程 $(s_0,s_1,s_2,...)$，有一个状态空间 $S$ 且 $|S|<\infty$，使得对于所有可能的马尔可夫过程的实现，都有 $s_i \in S$，$i=1,2,...$。

$\bullet$ 不变的状态转移概率（stationary transition probabilities）：状态转移概率与时间无关。从数学上讲，这意味着：

$$
P(s_i=s'|s_{i-1}=s) = P(s_j=s'|s_{j-1}=s), \forall s,s'\in S, \forall i,j=1,2,...。
\tag{1}
$$

除非另有说明，否则我们将认为这两条假设适用于本课程中提到的任何马尔可夫过程，包括后面将要涉及的马尔可夫奖励过程和马尔可夫决策过程。注意，满足这些假设的马尔可夫过程有时也被称作马尔科夫链（Markov chain），尽管马尔科夫链的精确定义与之不同。

对于马尔可夫过程，这些假设使得我们可以用一个矩阵来描述状态转移：转移概率矩阵（transition probability matrix）$\mathbf{P}$，其大小为 $|S|\times|S|$，第 $(i,j)$ 个元素为 $P_{ij}=P(j|i)$，这里 $i$ 和 $j$ 表示状态（随意排序）。注意，$\mathbf{P}$ 的元素是非负的，并且每行的和为 $1$。

因此，我们可以用 $(S,\mathbf{P})$ 来定义一个马尔可夫过程：

$\bullet$ $S$：一个有限的状态空间；

$\bullet$ $\mathbf{P}$：一个详列 $P(s'|s)$ 的状态转移概率模型。

**练习 3.1** <span id="ex3_1">（a）证明：$\mathbf{P}$ 是一个行随机的（row-stochastic）矩阵；（b）证明：$1$ 是行随机的矩阵的特征值，并找到一个对应的特征向量；（c）证明：行随机的矩阵的特征值的最大绝对值为 $1$。</span>

**练习 3.2** <span id="ex3_2"> 向量 $x\in \mathbb{R}^n$ 的最大范数（max-norm）或无穷范数（infinity-norm）记为 $\lVert x\rVert_{\infty}$，定义为 $\lVert x\rVert_{\infty} = \mathop{\max}_{i}|x_i|$，即为 $x$ 的元素的最大绝对值。对于矩阵 $\mathbf{A}\in \mathbb{R}^{m\times n}$，有如下定义：</span>

$$
\lVert \mathbf{A} \rVert_{\infty} = \mathop{\sup}_ {x\in\mathbb{R}^{n},x\neq 0} \frac{\lVert \mathbf{A}x\rVert_{\infty}}{\lVert x\rVert_{\infty}}。
\tag{2}
$$

（a）证明：$\lVert \mathbf{A}x\rVert_{\infty}$ 满足范数的所有性质。这样定义的量称为矩阵的诱导无穷范数（induced infinity norm）。

（b）证明：

$$
\lVert \mathbf{A}\rVert_{\infty} = \mathop{\max}_ {i=1,...,m}(\sum_{j=1}^{n}|A_{ij}|)。
\tag{3}
$$

（c）证明：若 $\mathbf{A}$ 是行随机的，那么 $\lVert \mathbf{A}\rVert_{\infty} = 1$。

（d）证明：对于所有 $x\in \mathbb{R}^{n}$，$\lVert \mathbf{A}x\rVert_{\infty} \leq \lVert \mathbf{A}\rVert_{\infty} \lVert x\rVert_{\infty}$。

#### 3.1.1 马尔可夫过程示例：火星探测器（Example of a Markov Process: Mars Rover）

考虑[图 1](#fig1) 所示的马尔可夫过程。我们的行为体是一个火星探测器，其状态空间为 $S={S1,S2,S3,S4,S5,S6,S7}$，状态转移概率在图中用箭头表示，例如，如果探测器在当前时间步处于状态 $S4$，那么在下个时间步状态为 $S3,S4,S5$ 的概率分别为 $0.4,0.2,0.4$。

假设探测器起始于状态 $S4$，那么一些可能的马尔可夫过程可以为：

$- S4,S5,S6,S7,S7,S7,...$

$- S4,S4,S5,S4,S5,S6,...$

<span id="fig1">$- S4,S3,S2,S1,...$</span>

**图1**

**练习 3.3** 考虑[图 1](#fig1) 中的马尔可夫过程例子。（a）写出其状态转移矩阵。

### 3.2 马尔可夫奖励过程（Markov Reward Process）

马尔可夫奖励过程是一个马尔可夫过程，并且有特定的奖励函数（reward function）和衰减系数（discount factor），通常由 $(S,\mathbf{P},R,\gamma)$ 表示：

$\bullet$ $S$：有限的状态空间；

$\bullet$ $\mathbf{P}$：状态转移模型；

$\bullet$ $R$：将状态映射到奖励（实数）的奖励函数，即 $R:S\to\mathbb{R}$；

$\bullet$ $S$：衰减系数，范围为 $[0,1]$。

我们已经解释了 $S$ 和 $\mathbf{P}$ 在马尔可夫过程中扮演的角色，接下来我们将解释奖励函数 $R$ 和衰减系数 $\gamma$ 的概念，它们是马尔可夫奖励过程特有的。此外，我们还将定义和解释一些在马尔可夫奖励过程中比较重要的量，例如时间范围（horizon）、回报（return）和状态值函数（state value function）。

#### 3.2.1 奖励函数（Reward Function）

在马尔可夫奖励过程中，当状态从当前状态 $s$ 转移到后续状态 $s'$ 时，根据当前状态 $s$ 获得奖励。因此对于马尔可夫过程 $(s_0,s_1,s_2,...)$，每个状态转移 $s_i\to s_{i+1}$ 都伴随着一个奖励 $r_i$（对于所有 $i=0,1,...$），因此，马尔可夫奖励过程可以表示为 $(s_0,r_0,s_1,r_1,s_2,r_2,...)$。注意，这些奖励可以是确定的，也可以是随机的。对于状态 $s\in S$，我们定义期望奖励为 $R(s)$：

$$
R(s) = \mathbb{E}[r_0|s_0=s]，
\tag{4}
$$

即 $R(s)$ 是当马尔可夫过程起始于状态 $s$ 时，第一次状态转移获得的奖励的期望值。正如前面假设的状态转移概率不变那样，我们将做出如下假设：

<span id="eq5">$\bullet$ 不变的奖励（stationary rewards）：马尔可夫奖励过程中的奖励是不变的意味着它们是时间无关的。在确定的奖励的情况下，数学上这表示对于过程的所有实现，一定有：</span>

$$
r_i=r_j, \quad \text{whenever}\text{ }s_i=s_j, \forall i,j=0,1,...，
\tag{5}
$$

<span id="eq6">在随机的奖励的情况下，我们要求以当前状态为条件的奖励的累积分布函数（cumulative distribution function，cdf）是时间无关的，数学上表示为：</span>

$$
F(r_i|s_i=s)=F(r_j|s_j=s), \quad \forall s\in S, \forall i,j=0,1,...，
\tag{6}
$$

<span id="eq7">这里 $F(r_i|s_i=s)$ 为以状态 $s_i=s$ 为条件时 $r_i$ 的累积分布函数。注意，根据式（[5](#eq5)）或式（[6](#eq6)），我们可以用下式来表示期望奖励：</span>

$$
R(s) = \mathbb{E}[r_i|s_i=s], \quad \forall i=0,1,...。
\tag{7}
$$

可以看到，只要马尔可夫奖励过程的“不变的奖励”的假设是真的，那么就只有期望奖励 $R$ 才是我们所关心的，并且我们可以不使用 $r_i$ 这个量。因此，以后“奖励”这个词将被交替使用来表示 $R$ 和 $r_i$，并且应该很容易通过上下文来理解。最后注意，在有限状态空间 $S$ 的情况下，$R$ 可以表示为一个维度为 $|S|$ 的向量。

**练习 3.4** （a）在假设状态转移概率和奖励不变的情况下，证明式（[7](#eq7)）。

#### 3.2.2 时间范围，回报和值函数（Horizon, Return and Value Function）

接下来我们定义马尔可夫奖励过程的时间范围、回报和值函数。

$\bullet$ 时间范围（Horizon）：马尔可夫奖励过程的时间范围 $H$ 定义为每个片段的时间步的数量。时间范围可以是有限的，也可以是无限的。如果时间范围是有限的，那么这个过程也称作有限马尔可夫奖励过程。

<span id="eq8">$\bullet$ 回报（Return）：马尔可夫奖励过程的回报 $G_t$ 定义为从时间 $t$ 开始，到时间范围 $H$ 为止所得到的衰减奖励和：</span>

$$
G_t = \sum_{i=t}^{H-1}\gamma^{i-t}r_i, \quad \forall 0\leq t\leq H-1。
\tag{8}
$$

<span id="eq9">$\bullet$ 状态值函数（State value function）：马尔可夫奖励过程的状态值函数 $V_t(s)$ 定义为状态 $s$ 在时间 $t$ 的期望回报：</span>

$$
V_t(s)=\mathbb{E}[G_t|s_t=s]。
\tag{9}
$$

<span id="eq10">注意，当 $H$ 是无限的时，通过式（[9](#eq9)）和奖励与状态转移概率不变的假设，我们可以得出：$V_i(s)=V_j(s)$，对于所有的 $i,j=0,1,...$，因此在这种情况下我们可以定义：</span>

$$
V(s) = V_0(s)。
\tag{10}
$$

**练习 3.5** （a）如果奖励和状态转移概率不变的假设成立，且时间范围 $H$ 是无限的，使用式（[8](#eq8)）和式（[9](#eq9)）证明 $V_i(s)=V_j(s)$，对于所有的 $i,j=0,1,...$。

#### 3.2.3 衰减系数（Discount Factor）

注意，式（[8](#eq8)）定义的回报 $G_t$ 中，如果时间范围是无限的并且 $\gamma=1$，那么即使奖励都是有界的，回报也可能是无限的。如果这样，那么值函数 $V(s)$ 也是无限的，计算机无法解决这种问题。为了避免这种数学困难并使问题在计算上易于处理，我们设置 $\gamma<1$，这样在按照式（[8](#eq8)）计算回报时，未来奖励的贡献度会指数型下降。这里 $\gamma$ 被称为衰减系数（discount factor）。除了纯粹的计算原因之外，注意，人类的行为方式几乎是一样的：我们倾向于把即时奖励放在比后续奖励更重要的位置。对 $\gamma$ 的解释是，当 $\gamma=0$ 时，我们只关心即时奖励，当 $\gamma=1$ 时，我们视即时奖励和未来奖励同等重要。最后注意，如果马尔可夫奖励过程的时间范围是有限的，即 $H<\infty$，那么我们可以设置 $\gamma=1$，因为回报和值函数总是有限的。

**练习 3.6** 考虑有限时间范围和有限奖励的马尔可夫奖励过程。假设 $\exists M\in(0,\infty)$，使得对于所有的片段（实现）以及 $\forall i$，都有 $|r_i|\leq M$。（a）证明：对于所有片段，式（[8](#eq8)）定义的回报 $G_t$ 是有界的；（b）能否找到一个 $C(M,\gamma,t,H)$ 使得 对于所有片段，都有 $|G_t|\leq C$？

**练习 3.7** 考虑无限时间范围、有限奖励和 $\gamma<1$ 的马尔可夫奖励过程。（a）证明：对于所有片段，式（[8](#eq8)）定义的回报 $G_t$ 收敛到一个有限值。（提示：考虑 $S_n=\sum_{i=t}^{N}\gamma^{i-t}r_i$，这里 $N\geq t$，证明 $\{S_N\}_{N\geq t}$ 是一个柯西数列。）

#### <span id="sec3_2_4">3.2.4 马尔可夫奖励过程示例：火星探测器（Example of a Markov Reward Process: Mars Rover）</span>

考虑[图 2](#fig2) 所示的马尔可夫奖励过程，其状态和状态转移概率和 **练习 3.3** 中的火星探测器例子完全相同。在 ${S2,S3,S4,S5,S6}$ 中的任一状态执行动作得到的奖励都为 $0$，在 $S1$ 执行动作的奖励为 $1$，在 $S7$ 执行动作的奖励为 $10$。奖励是不变且确定的。这个例子中假设 $\gamma=0.5$。

**图2**

我们再次建设探测器的初始状态为 $S4$。考虑有限时间范围的情况 $H=4$。一些可能的片段以及回报 $G_0$ 如下所示：

$- S4,S5,S6,S7,S7$：$G_0 = 0 + 0.5\times 0 + 0.5^2\times 0 + 0.5^3\times 10=1.25$

$- S4,S4,S5,S4,S5$：$G_0 = 0 + 0.5\times 0 + 0.5^2\times 0 + 0.5^3\times 0=0$

$- S4,S3,S2,S1,S2$：$G_0 = 0 + 0.5\times 0 + 0.5^2\times 0 + 0.5^3\times 1=0.125$

### 3.3 计算马尔可夫奖励过程的值函数（Computing the Value Function of a Markov Reward Process）

这一部分我们介绍三种计算马尔可夫奖励过程的值函数的方法：

$\bullet$ 仿真（Simulation）

$\bullet$ 解析解（Analytic solution）

$\bullet$ 迭代解（Iterative solution）

#### 3.3.1 蒙特卡洛仿真（Monte Carlo Simulation）

第一种方法是使用马尔可夫奖励过程的转移概率模型和奖励来生成大量的片段，对于每个片段可以计算其回报，然后对其求平均值得到平均回报。对于一个马尔可夫奖励过程 $M=(S,\mathbf{P},R,\gamma)$，状态 $s$，时间 $t$，以及片段仿真次数 $N$，仿真算法的伪代码如算法 1 所示。

**算法1**

#### 3.3.2 解析解（Analytic Solution）

<span id="eq11">这种方法只适用于时间范围无限且 $\gamma < 1$ 的马尔可夫奖励过程。通过式（[9](#eq9)）以及前面的假设，对于 $\forall s\in S$ 有</span>

$$
V(s) \overset{(a)}{=} V_0(s) = \mathbb{E}[G_0|s_0=s] = \mathbb{E}[\sum_{i=0}^{\infty}\gamma^i r_i|s_0=s] = \mathbb{E}[r_0|s_0=s] + \sum_{i=1}^{\infty}\gamma^i \mathbb{E}[r_i|s_0=s]
$$

$$
\overset{(b)}{=} \mathbb{E}[r_0|s_0=s]+\sum_{i=1}^{\infty}\gamma^i(\sum_{s'\in S}P(s_1=s'|s_0=s)\mathbb{E}[r_i|s_0=s,s_1=s'])
$$

$$
\overset{(c)}{=} \mathbb{E}[r_0|s_0=s]+\gamma \sum_{s'\in S}P(s'|s)\mathbb{E}[\sum_{i=0}^{\infty}\gamma^i r_i|s_0=s']
$$

$$
\overset{(d)}{=} R(s)+\gamma\sum_{s'\in S}P(s'|s)V(s')，
\tag{11}
$$

<span id="eq12">这里 $(a)$ 式（[8](#eq8)）、式（[9](#eq9)）和式（[10](#eq10)）得到，$(b)$由期望的定义得到，$(c)$ 由马尔可夫性质和不变性假设得到，$(d)$ 由式（[4](#eq4)）得到。对于式（[11](#eq11)）的最终结果有一个很好的解释：第一项 $R(s)$ 为即时奖励，第二项 $\gamma\sum_{s'\in S}P(s'|s)V(s')$ 为未来奖励的衰减累加，值函数 $V(s)$ 是这两项的和。因为 $|S|<\infty$，我们可以用矩阵的形式重新表述这个等式：</span>

$$
V=R+\gamma \mathbf{P}V，
\tag{12}
$$

这里 $\mathbf{P}$ 为前面介绍的状态转移矩阵，$R$ 和 $V$ 为两个维度为 $|S|$ 的列向量，分别包含所有的 $R(s)$ 和 $V(s)$。式（[12](#eq12)）可以被写为 $(\mathbf{I}-\gamma\mathbf{P})V=R$，则解析解为 $V=(\mathbf{I}-\gamma\mathbf{P})^{-1}R$。注意 $\gamma < 1$ 且 $\mathbf{P}$ 是行随机的，因此 $(\mathbf{I}-\gamma\mathbf{P})$ 是非奇异的（可逆的）。因此式（[12](#eq12)）总是有解并且解是唯一的。然而，由于需要求逆矩阵，这个方法的计算复杂度是 $O(|S|^3)$，因此并不适合状态空间很大的情况。

**练习 3.8** 考虑矩阵 $(\mathbf{I}-\gamma\mathbf{P})$。（a）证明：$1-\gamma$ 是该矩阵的特征值，并找到一个对应的特征向量；（b）对于 $0<\gamma<1$，应用 [**练习 3.1**](#ex3_1) 的结论来证明 $(\mathbf{I}-\gamma\mathbf{P})$ 是非奇异的（可逆的）。

**练习 3.9** 考虑 [**3.2.4**](#sec3_2_4) 小节中的马尔可夫奖励过程。（a）如果 $H$ 是无穷的，计算所有状态的值函数。

#### 3.3.3 迭代解（Iterative Solution）

我们现在给出在无限时间范围情况（且 $\gamma<1$）下评估值函数的迭代解和有限时间范围情况下的基于动态规划的解。令人惊讶的是，两种算法看起来非常相似，以至于很难分辨出差异。我们首先考虑有限时间范围的情况，很容易证明（与式（[11](#eq11)）的证明类似）在这种情况下：

$$
V_t(s)=R(s)+\gamma\sum_{s'\in S}P(s'|s)V_{t+1}(s'), \quad \forall t=0,...,H-1，
$$

$$
V_H(s)=0。
\tag{13}
$$

**练习 3.10** 证明：式（[13](#eq13)）符合有限时间范围的马尔可夫奖励过程。

这些式子提供了解决动态规划的方法，其伪代码见算法 2。该算法将有限时间范围的马尔可夫奖励过程 $M=(S,\mathbf{P},R,\gamma)$ 作为输入，计算所有状态在所有时间的值函数。

**算法2**

现在我们考虑对于无限时间范围且 $\gamma<1$ 的情况的迭代算法，伪代码见算法 3。该算法将马尔可夫奖励过程 $M=(S,\mathbf{P},R,\gamma)$、容限 $\epsilon$ 作为输入，计算所有状态的值函数。

**算法3**

对于算法 2 和 算法 3，每个循环的计算消耗是 $O(|S|^2)$，这相对于无限时间范围情况下的解析方法的消耗 $O(|S|^3)$ 是一个提升，然而我们可能需要很多次迭代来满足收敛条件 $\epsilon$。

尽管在有限时间范围情况下容易算法 2 的正确性，但对于无限时间范围的情况，算法 3 是否收敛，以及是否收敛到正确的解 $(\mathbf{I}-\gamma \mathbf{P})^{-1}R$ 就不那么清楚了。这两个问题的回答都是肯定的，如下定理所示。

**定理 3.1** <span id="thm3_1"> 算法 3 总是会终止。此外，如果算法 3 的输出为 $V'$，并且将真正的解记为 $V=(\mathbf{I}-\gamma \mathbf{P})^{-1}R$，那么 $\Vert V'-V \Vert_{\infty}\leq\frac{\epsilon\gamma}{1-\gamma}$。</span>

证明：考虑包含了 $\Vert\cdot\Vert_{\infty}$ 范数的向量空间 $\mathbb{R}^{|S|}$（参考 [**练习 3.2**](#ex3_2)），这样构造的 $\mathbb{R}^{|S|}$ 是一个 Banach 空间（关于范数向量空间，参考 [附录A](#appendix_a)）。首先，$V$ 和 $V'$ 以及算法 3 中的迭代都是 $\mathbb{R}^{|S|}$ 的元素。

对于 $U\in\mathbb{R}^{|S|}$，定义运算 $B:\mathbb{R}^{|S|}\to\mathbb{R}^{|S|}$：

$$
(BU)(s)=R(s)+\gamma\sum_{s'\in S}P(s'|s)U(s'), \quad \forall s\in S，
\tag{14}
$$

<span id="eq15">这个运算可以写成以下紧凑的形式：</span>

$$
BU = R + \gamma \mathbf{P}U。
\tag{15}
$$

我们首先证明运算 $B$ 是严格收缩（在 [**定义 A.3**](#def_A3) 中定义）。对于所有的 $U_1,U_2\in\mathbb{R}^{|S|}$，根据式（[15](#eq15)）我们有

$$
\Vert BU_1-BU_2 \Vert_{\infty} = \gamma\Vert \mathbf{P}U_1-\mathbf{P}U_2\Vert_{\infty} = \gamma\Vert \mathbf{P}(U_1-U_2)\Vert_{\infty}
$$

$$
\leq \gamma\Vert \mathbf{P}\Vert_{\infty}\Vert U_1-U_2\Vert_{\infty} = \gamma\Vert U_1-U_2\Vert_{\infty}，
\tag{16}
$$

这里第二步可以根据 [**练习 3.2**](#ex3_2) 得到。由于 $0<\gamma<1$，可以推出 $B$ 是 $\mathbb{R}^{|S|}$ 上的严格收缩。因此，根据收缩映射定理（[**定理 A.5**](#thm_A5)），可以推出 $B$ 有唯一的不动点。根据式（[15](#eq15)）和式（[12](#eq12)），还可以得知 $BV=R+\gamma\mathbf{P}V=V$，因此 $V$ 是 $B$ 的一个不动点，根据唯一性，$V$ 就是那个唯一的不动点。

然后我们考虑算法 3 中的迭代，将它们定义为 $\lbrace V_k\rbrace_{k\geq 1}$。注意，它们满足下列关系：

$$
V_k = 
\begin{cases}
0,  & \text{if $k=1$} \\
BV_{k-1}, & \text{if $k>1$}
\end{cases}
\tag{17}
$$

根据 [**定理 A.5**](#thm_A5) 可知 $\lbrace V_k\rbrace_{k\geq 1}$ 是一个柯西序列，因此根据 [**定义 A.1**](#def_A1) 可知存在 $N\geq 1$，使得对于所有 $m,n> N$，都有 $\Vert V_m-V_n\Vert_{\infty}<\epsilon$。这证明算法 3 会终止。注意，收缩映射定理（[**定理 A.5**](#thm_A5)）还意味着 $V_k\to V$（关于收敛的概念，参考[**定义 A.2**](#def_A2)）。

为了证明算法终止时的误差范围，令算法在 $k$ 次迭代后停止，最后一次迭代后得到 $V_{k+1}$。那么我们有 $\Vert V_{k+1}-V_k \Vert_{\infty}\leq\epsilon$。根据三角不等式以及 $V_{k+1}=BV_k$，可以推出

$$
\Vert V_k-V \Vert_{\infty}\leq\Vert V_{k}-V_{k+1} \Vert_{\infty} + \Vert V_{k+1}-V \Vert_{\infty} = \Vert V_{k}-V_{k+1} \Vert_{\infty} + \Vert BV_k - BV \Vert_{\infty}
$$

$$
\leq \Vert V_{k}-V_{k+1} \Vert_{\infty} + \gamma \Vert V_k-V \Vert_{\infty} = \epsilon + \gamma \Vert V_k-V \Vert_{\infty}，
\tag{18}
$$

因此 $\Vert V_k-V \Vert_{\infty} \leq \frac{\epsilon}{1-\gamma}$。最终，我们可以推出

$$
\Vert V_{k+1}-V \Vert_{\infty} = \Vert BV_k-BV \Vert_{\infty} \leq \gamma \Vert V_{k}-V \Vert_{\infty} \leq \frac{\epsilon\gamma}{1-\gamma}。
\tag{19}
$$

证明完毕。$\diamondsuit$

**练习 3.11** 假设在算法 3 中，初始化时 $V'$ 被随机设置（所有元素都是有限的）而不是 $V' \leftarrow 0$。（1）算法仍会收敛吗？（2）算法仍然保持和 [**定理 3.1**](#thm3_1) 相同的误差吗？

**练习 3.12** 假设 [**定理 3.1**](#thm3_1) 的假设成立，使用和 [**定理 3.1**](#thm3_1) 相同的符号证明：（1）对于所有的 $k\geq 1$，$\Vert V_k-V\Vert_{\infty}\leq \gamma^{k-1}\Vert V\Vert_{\infty}$；（2）$\Vert V_2 \Vert_{\infty}\leq (1+\gamma)\Vert V\Vert_{\infty}$；（3）对于所有的 $m,n\geq 1$，$\Vert V_m-V_n\Vert_{\infty}\leq (\gamma^{m-1}+\gamma^{n-1})\Vert V\Vert_{\infty}$。

### 3.4 马尔可夫决策过程（Markov Decision Process）

现在我们定义马尔可夫决策过程（Markov decision process, MDP）。MDP 继承了马尔可夫奖励过程的基本结构，在 MDP 中，行为体可以在每个状态下采取某些动作。MDP 可以用 $(S,A,P,R,\gamma)$ 来正式表示：

$\bullet\quad S$：一个有限的状态空间；

$\bullet\quad A$：一个有限的动作空间；

$\bullet\quad P$：一个描述 $P(s'|s,a)$ 的状态转移概率模型；

$\bullet\quad R$：一个将状态-动作对映射到奖励的奖励函数，即 $R:S\times A\to R$；

$\bullet\quad \gamma$：衰减因子 $\gamma\in [0,1]$。

其中一些量已经在马尔可夫奖励过程中解释，然而在 MDP 中，我们需要提及一些重要的区别。动力学的基本模型包含了一个状态空间 $S$ 和一个动作空间 $A$，我们将这两个空间视为有限的空间。行为体在时间 $i$ 从一个状态 $s_i$ 开始，从动作空间中选择一个动作 $a_i$，得到一个奖励 $r_i$ 并转移到下一个状态 $s_{i+1}$。因此，MDP 中的一个片段可以被描述为 $(s_0,a_0,r_0,s_1,a_1,r_1,s_2,a_2,r_2,\cdots)$。

与在马尔可夫过程和马尔可夫奖励过程中，状态转移概率仅是下一状态以及当前状态的函数，而在 MDP 中，在时间 $i$ 的状态转移是下一状态 $s_{i+1}$、当前状态 $s_i$ 以及动作 $a_i$ 的函数，写作 $P(s_{i+1}|s_i,a_i)$。我们仍假设状态转移概率不变，这在 MDP 中可以写为

$$
P(s_i=s'|s_{i-1}=s,a_{i-1}=a) = P(s_j=s'|s_{j-1}=s,a_{j-1}=a)，
\tag{20}
$$

对于所有 $s,s'\in S$，所有 $a\in A$ 以及所有 $i,j=1,2,\cdots$。

在 MDP 中，在时间 $i$ 的奖励 $r_i$ 依赖于 $s_i$ 和 $a_i$，而在马尔可夫奖励过程中，奖励只依赖与当前状态。MDP 中的奖励可以是随机的或确定的，但就像马尔可夫奖励过程那样，我们假设奖励是确定的，并且唯一相关的量是期望奖励，对于固定的状态 $s$ 和动作 $a$，我们将这个期望奖励记为 $R(s,a)$：

$$
R(s,a) = \mathbb{E}[r_i|s_i=s,a_i=a],\quad \forall i=0,1,\cdots。
\tag{21}
$$

MDP 中的衰减引子 $\gamma$，时间范围 $H$ 以及回报 $G_t$ 的概念与马尔可夫奖励过程中的概念完全相同。然而，MDP 中的状态值函数稍有不同，如下所述。

#### 3.4.1 MDP 策略与策略评估（MDP Policies and Policy Evaluation）

给定一个 MDP，这个 MDP 的策略指定在每个状态下要执行的动作。策略可以是确定的或随机的，我们将考虑概率分布式的策略来同时包含这两种情况。需要注意的是，策略可能会随时间而变化，尤其是在有限时间范围的 MDP 中。我们将用 $\boldsymbol{\pi}$ 来表示一个通用的策略，定义为无限维数组 $\boldsymbol{\pi}=(\pi_0,\pi_1,\cdots)$，这里 $\pi_t$ 表示在时间 $t$ 的策略。我们称不随时间变化的策略为固定策略（stationary policies）并将其表示为 $\pi$，即在这种情况下 $\boldsymbol{\pi}=(\pi,\pi,\cdots)$。对于固定策略 $\pi$，如果在时间 $t$，行为体处于状态 $s$，它将选择一个概率为 $\pi(a|s)$ 的动作 $a$，并且这个概率不依赖于 $t$；对于非固定策略，这个概率将依赖于 $t$，我们用 $\pi_t(a|s)$ 来表示。

对于一个 MDP，给定一个策略 $\boldsymbol{\pi}$，我们可以定义两个量：状态值函数（state value function）和状态-动作值函数（state-action value function）：

<span id='eq22'>$\bullet$</span> 状态值函数：对于状态 $s\in S$，状态值函数 $V_t^{\boldsymbol{\pi}}(s)$ 定义为在时间 $t$ 起始于状态 $s_t =s$，遵循策略 $\boldsymbol{\pi}}$ 的期望回报：$V_t^{\boldsymbol{\pi}}(s) = \mathbb{E}_{\boldsymbol{\pi}}[G_t|s_t=s]$，这里 $\mathbb{E}_{\boldsymbol{\pi}}$ 表示关于策略 $\boldsymbol{\pi}$ 的期望。通常在期望的表达式中我们省略 $\boldsymbol{\pi}$，因此除非特殊强调，$\mathbb{E}$ 将表示 $\boldsymbol{\pi}$：

$$
V_t^{\boldsymbol{\pi}}(s) = \mathbb{E}[G_t|s_t=s]。
\tag{22}
$$

注意，当时间范围 $H$ 无限时，定义（[22](#eq22)）以及奖励、转移概率和策略的不变假设意味着对于所有的 $s\in S$，$V_i^{\boldsymbol{\pi}}(s)=V_j^{\boldsymbol{\pi}}(s)$，这里 $i,j=0,1,\cdots$，因此在这种情况下，我们将以类似于马尔可夫奖励过程的情况来做以下定义：

$$
V^{\pi}(s)=V_0^{\boldsymbol{\pi}}(s)。
\tag{23}
$$