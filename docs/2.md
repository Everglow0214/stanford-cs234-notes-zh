# Lecture 2 Making Good Decisions Given a Model of the World

# 课时2 已知环境模型时，如何做出好的决策 2019.01.09

## 3. 在马尔可夫决策过程中做出动作（Acting in a Markov decision process）

我们从回忆模型（model）、策略（policy）和值函数（value function）的定义开始。令行为体的状态空间和动作空间分别表示为 $S$ 和 $A$，那么，

$\bullet$ 模型：模型是行为体所处的环境的状态转移与奖励的数学模型，包括状态转移概率 P(s'|s,a)，表示状态 $s\in S$ 在执行动作 $a\in A$ 后，状态转移到 $s'\in S$ 后的概率，以及在状态 $s\in S$ 执行动作 $a\in A$ 后得到的奖励 $R(s,a)$（确定的或随机的）。

$\bullet$ 策略：策略是一个将行为体的状态映射到动作的函数 $\pi:S\rightarrow A$。

$\bullet$ 值函数：对应于特定策略 $\pi$ 和状态 $s\in S$ 的值函数 $V^{\pi}$ 是行为体从状态 $s$ 开始，遵循策略 $\pi$ 所能获得的未来（衰减）奖励的累加。

同时回忆一下上节课提到的马尔可夫性质（Markov property）的概念。考虑一个遵循某个转移规律的随机过程 $(s_0,s_1,s_2,...)$，我们称这个随机过程有马尔可夫性质当且仅当对于 $\forall i=1,2,...$，$P(s_i|s_0,...,s_{i-1})=P(s_i|s_{i-1})$，即以包括当前状态在内的历史为条件的转移到下一状态的概率，与仅以当前状态为条件的转移到下一状态的概率相等。在这种情况下，当前状态是随机过程的历史的一个充分统计，我们认为“未来与过去无关”。

本课中，我们将在这些定义的基础上，首先定义马尔可夫过程（Markov process，MP），然后定义马尔可夫回报过程（Markov reward process，MRP），最后定义马尔可夫决策过程（Markov decision process，MDP）。我们将通过讨论一些算法来结束本课，这些算法使我们能够在 MDP 完全已知的情况下做出好的决策。

### 3.1 马尔可夫过程（Markov Process）

在最一般的情况下，马尔可夫过程是一个满足马尔可夫性质的随机过程，因此我们称马尔可夫过程是“无记忆的”。在本课中，我们将提出两个在强化学习设定中非常常见的假设：

$\bullet$ 有限的状态空间（finite state space）：马尔可夫过程的状态空间是有限的。这意味着对于马尔可夫过程 $(s_0,s_1,s_2,...)$，有一个状态空间 $S$ 且 $|S|<\infty$，使得对于所有可能的马尔可夫过程的实现，都有 $s_i \in S$，$i=1,2,...$。

$\bullet$ 不变的状态转移概率（stationary transition probabilities）：状态转移概率与时间无关。从数学上讲，这意味着：

$$
P(s_i=s'|s_{i-1}=s) = P(s_j=s'|s_{j-1}=s), \forall s,s'\in S, \forall i,j=1,2,...。
\tag{1}
$$

除非另有说明，否则我们将认为这两条假设适用于本课程中提到的任何马尔可夫过程，包括后面将要涉及的马尔可夫奖励过程和马尔可夫决策过程。注意，满足这些假设的马尔可夫过程有时也被称作马尔科夫链（Markov chain），尽管马尔科夫链的精确定义与之不同。

对于马尔可夫过程，这些假设使得我们可以用一个矩阵来描述状态转移：转移概率矩阵（transition probability matrix）$\mathbf{P}$，其大小为 $|S|\times|S|$，第 $(i,j)$ 个元素为 $P_{ij}=P(j|i)$，这里 $i$ 和 $j$ 表示状态（随意排序）。注意，$\mathbf{P}$ 的元素是非负的，并且每行的和为 $1$。

因此，我们可以用 $(S,\mathbf{P})$ 来定义一个马尔可夫过程：

$\bullet$ $S$：一个有限的状态空间；

$\bullet$ $\mathbf{P}$：一个详列 $P(s'|s)$ 的状态转移概率模型。

**练习 3.1** （a）证明：$\mathbf{P}$ 是一个行随机的（row-stochastic）矩阵；（b）证明：$1$ 是行随机的矩阵的特征值，并找到一个对应的特征向量；（c）证明：行随机的矩阵的特征值的最大绝对值为 $1$。

**练习 3.2** 向量 $x\in \mathbb{R}^n$ 的最大范数（max-norm）或无穷范数（infinity-norm）记为 $\lVert x\rVert_{\infty}$，定义为 $\lVert x\rVert_{\infty} = \mathop{\max}_{i}|x_i|$，即为 $x$ 的元素的最大绝对值。对于矩阵 $\mathbf{A}\in \mathbb{R}^{m\times n}$，有如下定义：

$$
\lVert \mathbf{A} \rVert_{\infty} = \mathop{\sup}_ {x\in\mathbb{R}^{n},x\neq 0} \frac{\lVert \mathbf{A}x\rVert_{\infty}}{\lVert x\rVert_{\infty}}。
\tag{2}
$$

（a）证明：$\lVert \mathbf{A}x\rVert_{\infty}$ 满足范数的所有性质。这样定义的量称为矩阵的诱导无穷范数（induced infinity norm）。

（b）证明：

$$
\lVert \mathbf{A}\rVert_{\infty} = \mathop{\max}_ {i=1,...,m}(\sum_{j=1}^{n}|A_{ij}|)。
\tag{3}
$$

（c）证明：若 $\mathbf{A}$ 是行随机的，那么 $\lVert \mathbf{A}\rVert_{\infty} = 1$。

（d）证明：对于所有 $x\in \mathbb{R}^{n}$，$\lVert \mathbf{A}x\rVert_{\infty} \leq \lVert \mathbf{A}\rVert_{\infty} \lVert x\rVert_{\infty}$。

#### 3.1.1 马尔可夫过程示例：火星探测器（Example of a Markov Process: Mars Rover）

考虑[图 1](#fig1) 所示的马尔可夫过程。我们的行为体是一个火星探测器，其状态空间为 $S={S1,S2,S3,S4,S5,S6,S7}$，状态转移概率在图中用箭头表示，例如，如果探测器在当前时间步处于状态 $S4$，那么在下个时间步状态为 $S3,S4,S5$ 的概率分别为 $0.4,0.2,0.4$。

假设探测器起始于状态 $S4$，那么一些可能的马尔可夫过程可以为：

$- S4,S5,S6,S7,S7,S7,...$

$- S4,S4,S5,S4,S5,S6,...$

$- S4,S3,S2,S1,...$

**图1**

**练习 3.3** 考虑[图 1](#fig1) 中的马尔可夫过程例子。（a）写出其状态转移矩阵。

### 3.2 马尔可夫奖励过程（Markov Reward Process）

马尔可夫奖励过程是一个马尔可夫过程，并且有特定的奖励函数（reward function）和衰减因子（discount factor），通常由 $(S,\mathbf{P},R,\gamma)$ 表示：

$\bullet$ $S$：有限的状态空间；

$\bullet$ $mathbf{P}$：状态转移模型；

$\bullet$ $R$：将状态映射到奖励（实数）的奖励函数，即 $R:S\to\mathbb{R}$；

$\bullet$ $S$：衰减因子，范围为 $[0,1]$。

我们已经解释了 $S$ 和 $\mathbf{P}$ 在马尔可夫过程中扮演的角色，接下来我们将解释奖励函数 $R$ 和衰减因子 $\gamma$ 的概念，它们是马尔可夫奖励过程特有的。此外，我们还将定义和解释一些在马尔可夫奖励过程中比较重要的量，例如时间范围（horizon）、回报（return）和状态值函数（state value function）。

#### 3.2.1 奖励函数（Reward Function）