# Lecture 2 Making Good Decisions Given a Model of the World

# 课时2 已知环境模型时，如何做出好的决策 2019.01.09

## 3. 在马尔可夫决策过程中做出动作（Acting in a Markov decision process）

我们从回忆模型（model）、策略（policy）和值函数（value function）的定义开始。令行为体的状态空间和动作空间分别表示为 $S$ 和 $A$，那么，

$\bullet$ 模型：模型是行为体所处的环境的状态转移与奖励的数学模型，包括状态转移概率 P(s'|s,a)，表示状态 $s\in S$ 在执行动作 $a\in A$ 后，状态转移到 $s'\in S$ 后的概率，以及在状态 $s\in S$ 执行动作 $a\in A$ 后得到的奖励 $R(s,a)$（确定的或随机的）。

$\bullet$ 策略：策略是一个将行为体的状态映射到动作的函数 $\pi:S\rightarrow A$。

$\bullet$ 值函数：对应于特定策略 $\pi$ 和状态 $s\in S$ 的值函数 $V^{\pi}$ 是行为体从状态 $s$ 开始，遵循策略 $\pi$ 所能获得的未来（衰减）奖励的累加。

同时回忆一下上节课提到的马尔可夫性质（Markov property）的概念。考虑一个遵循某个转移规律的随机过程 $(s_0,s_1,s_2,...)$，我们称这个随机过程有马尔可夫性质当且仅当对于 $\forall i=1,2,...$，$P(s_i|s_0,...,s_{i-1})=P(s_i|s_{i-1})$，即以包括当前状态在内的历史为条件的转移到下一状态的概率，与仅以当前状态为条件的转移到下一状态的概率相等。在这种情况下，当前状态是随机过程的历史的一个充分统计，我们认为“未来与过去无关”。

本课中，我们将在这些定义的基础上，首先定义马尔可夫过程（Markov process，MP），然后定义马尔可夫回报过程（Markov reward process，MRP），最后定义马尔可夫决策过程（Markov decision process，MDP）。我们将通过讨论一些算法来结束本课，这些算法使我们能够在 MDP 完全已知的情况下做出好的决策。

### 3.1 马尔可夫过程（Markov Process）

在最一般的情况下，马尔可夫过程是一个满足马尔可夫性质的随机过程，因此我们称马尔可夫过程是“无记忆的”。在本课中，我们将提出两个在强化学习设定中非常常见的假设：

$\bullet$ 有限的状态空间（finite state space）：马尔可夫过程的状态空间是有限的。这意味着对于马尔可夫过程 $(s_0,s_1,s_2,...)$，有一个状态空间 $S$ 且 $|S|<\infty$，使得对于所有可能的马尔可夫过程的实现，都有 $s_i \in S$，$i=1,2,...$。

$\bullet$ 不变的状态转移概率（stationary transition probabilities）：状态转移概率与时间无关。从数学上讲，这意味着：

$$
P(s_i=s'|s_{i-1}=s) = P(s_j=s'|s_{j-1}=s), \forall s,s'\in S, \forall i,j=1,2,...。
\tag{1}
$$

除非另有说明，否则我们将认为这两条假设适用于本课程中提到的任何马尔可夫过程，包括后面将要涉及的马尔可夫奖励过程和马尔可夫决策过程。注意，满足这些假设的马尔可夫过程有时也被称作马尔科夫链（Markov chain），尽管马尔科夫链的精确定义与之不同。

对于马尔可夫过程，这些假设使得我们可以用一个矩阵来描述状态转移：转移概率矩阵（transition probability matrix）$\mathbf{P}$，其大小为 $|S|\times|S|$，第 $(i,j)$ 个元素为 $P_{ij}=P(j|i)$，这里 $i$ 和 $j$ 表示状态（随意排序）。注意，$\mathbf{P}$ 的元素是非负的，并且每行的和为 $1$。