# Lecture 2 Making Good Decisions Given a Model of the World

# 课时2 已知环境模型时，如何做出好的决策 2019.01.09

## 3. 在马尔可夫决策过程中做出动作（Acting in a Markov decision process）

我们从回忆模型（model）、策略（policy）和值函数（value function）的定义开始。令行为体的状态空间和动作空间分别表示为 $S$ 和 $A$，那么，

$\bullet$ 模型：模型是行为体所处的环境的状态转移与奖励的数学模型，包括状态转移概率 P(s'|s,a)，表示状态 $s\in S$ 在执行动作 $a\in A$ 后，状态转移到 $s'\in S$ 后的概率，以及在状态 $s\in S$ 执行动作 $a\in A$ 后得到的奖励 $R(s,a)$（确定的或随机的）。

$\bullet$ 策略：策略是一个将行为体的状态映射到动作的函数 $\pi:S\rightarrow A$。

$\bullet$ 值函数：对应于特定策略 $\pi$ 和状态 $s\in S$ 的值函数 $V^{\pi}$ 是行为体从状态 $s$ 开始，遵循策略 $\pi$ 所能获得的未来（衰减）奖励的累加。

同时回忆一下上节课提到的马尔可夫性质（Markov property）的概念。考虑一个遵循某个转移规律的随机过程 $(s_0,s_1,s_2,...)$，我们称这个随机过程有马尔可夫性质当且仅当对于 $\forall i=1,2,...$，$P(s_i|s_0,...,s_{i-1})=P(s_i|s_{i-1})$，即以包括当前状态在内的历史为条件的转移到下一状态的概率，与仅以当前状态为条件的转移到下一状态的概率相等。在这种情况下，当前状态是随机过程的历史的一个充分统计，我们认为“未来与过去无关”。

本课中，我们将在这些定义的基础上，首先定义马尔可夫过程（Markov process，MP），然后定义马尔可夫回报过程（Markov reward process，MRP），最后定义马尔可夫决策过程（Markov decision process，MDP）。我们将通过讨论一些算法来结束本课，这些算法使我们能够在 MDP 完全已知的情况下做出好的决策。

### 3.1 马尔可夫过程（Markov Process）

在最一般的情况下，马尔可夫过程是一个满足马尔可夫性质的随机过程，因此我们称马尔可夫过程是“无记忆的”。在本课中，我们将提出两个在强化学习设定中非常常见的假设：

$\bullet$ 有限的状态空间（finite state space）：马尔可夫过程的状态空间是有限的。这意味着对于马尔可夫过程 $(s_0,s_1,s_2,...)$，有一个状态空间 $S$ 且 $|S|<\infty$，使得对于所有可能的马尔可夫过程的实现，都有 $s_i \in S$，$i=1,2,...$。

$\bullet$ 不变的状态转移概率（stationary transition probabilities）：状态转移概率与时间无关。从数学上讲，这意味着：

$$
P(s_i=s'|s_{i-1}=s) = P(s_j=s'|s_{j-1}=s), \forall s,s'\in S, \forall i,j=1,2,...。
\tag{1}
$$

除非另有说明，否则我们将认为这两条假设适用于本课程中提到的任何马尔可夫过程，包括后面将要涉及的马尔可夫奖励过程和马尔可夫决策过程。注意，满足这些假设的马尔可夫过程有时也被称作马尔科夫链（Markov chain），尽管马尔科夫链的精确定义与之不同。

对于马尔可夫过程，这些假设使得我们可以用一个矩阵来描述状态转移：转移概率矩阵（transition probability matrix）$\mathbf{P}$，其大小为 $|S|\times|S|$，第 $(i,j)$ 个元素为 $P_{ij}=P(j|i)$，这里 $i$ 和 $j$ 表示状态（随意排序）。注意，$\mathbf{P}$ 的元素是非负的，并且每行的和为 $1$。

因此，我们可以用 $(S,\mathbf{P})$ 来定义一个马尔可夫过程：

$\bullet$ $S$：一个有限的状态空间；

$\bullet$ $\mathbf{P}$：一个详列 $P(s'|s)$ 的状态转移概率模型。

**练习 3.1** （a）证明：$\mathbf{P}$ 是一个行随机的（row-stochastic）矩阵；（b）证明：$1$ 是行随机的矩阵的特征值，并找到一个对应的特征向量；（c）证明：行随机的矩阵的特征值的最大绝对值为 $1$。

**练习 3.2** 向量 $x\in \mathbb{R}^n$ 的最大范数（max-norm）或无穷范数（infinity-norm）记为 $\lVert x\rVert_{\infty}$，定义为 $\lVert x\rVert_{\infty} = \mathop{\max}_{i}|x_i|$，即为 $x$ 的元素的最大绝对值。对于矩阵 $\mathbf{A}\in \mathbb{R}^{m\times n}$，有如下定义：

$$
\lVert \mathbf{A} \rVert_{\infty} = \mathop{\sup}_ {x\in\mathbb{R}^{n},x\neq 0} \frac{\lVert \mathbf{A}x\rVert_{\infty}}{\lVert x\rVert_{\infty}}。
\tag{2}
$$

（a）证明：$\lVert \mathbf{A}x\rVert_{\infty}$ 满足范数的所有性质。这样定义的量称为矩阵的诱导无穷范数（induced infinity norm）。

（b）证明：

$$
\lVert \mathbf{A}\rVert_{\infty} = \mathop{\max}_ {i=1,...,m}(\sum_{j=1}^{n}|A_{ij}|)。
\tag{3}
$$

（c）证明：若 $\mathbf{A}$ 是行随机的，那么 $\lVert \mathbf{A}\rVert_{\infty} = 1$。

（d）证明：对于所有 $x\in \mathbb{R}^{n}$，$\lVert \mathbf{A}x\rVert_{\infty} \leq \lVert \mathbf{A}\rVert_{\infty} \lVert x\rVert_{\infty}$。

#### 3.1.1 马尔可夫过程示例：火星探测器（Example of a Markov Process: Mars Rover）

考虑[图 1](#fig1) 所示的马尔可夫过程。我们的行为体是一个火星探测器，其状态空间为 $S={S1,S2,S3,S4,S5,S6,S7}$，状态转移概率在图中用箭头表示，例如，如果探测器在当前时间步处于状态 $S4$，那么在下个时间步状态为 $S3,S4,S5$ 的概率分别为 $0.4,0.2,0.4$。

假设探测器起始于状态 $S4$，那么一些可能的马尔可夫过程可以为：

$- S4,S5,S6,S7,S7,S7,...$

$- S4,S4,S5,S4,S5,S6,...$

<span id="fig1">$- S4,S3,S2,S1,...$</span>

**图1**

**练习 3.3** 考虑[图 1](#fig1) 中的马尔可夫过程例子。（a）写出其状态转移矩阵。

### 3.2 马尔可夫奖励过程（Markov Reward Process）

马尔可夫奖励过程是一个马尔可夫过程，并且有特定的奖励函数（reward function）和衰减系数（discount factor），通常由 $(S,\mathbf{P},R,\gamma)$ 表示：

$\bullet$ $S$：有限的状态空间；

$\bullet$ $\mathbf{P}$：状态转移模型；

$\bullet$ $R$：将状态映射到奖励（实数）的奖励函数，即 $R:S\to\mathbb{R}$；

$\bullet$ $S$：衰减系数，范围为 $[0,1]$。

我们已经解释了 $S$ 和 $\mathbf{P}$ 在马尔可夫过程中扮演的角色，接下来我们将解释奖励函数 $R$ 和衰减系数 $\gamma$ 的概念，它们是马尔可夫奖励过程特有的。此外，我们还将定义和解释一些在马尔可夫奖励过程中比较重要的量，例如时间范围（horizon）、回报（return）和状态值函数（state value function）。

#### 3.2.1 奖励函数（Reward Function）

在马尔可夫奖励过程中，当状态从当前状态 $s$ 转移到后续状态 $s'$ 时，根据当前状态 $s$ 获得奖励。因此对于马尔可夫过程 $(s_0,s_1,s_2,...)$，每个状态转移 $s_i\to s_{i+1}$ 都伴随着一个奖励 $r_i$（对于所有 $i=0,1,...$），因此，马尔可夫奖励过程可以表示为 $(s_0,r_0,s_1,r_1,s_2,r_2,...)$。注意，这些奖励可以是确定的，也可以是随机的。对于状态 $s\in S$，我们定义期望奖励为 $R(s)$：

$$
R(s) = \mathbb{E}[r_0|s_0=s]，
\tag{4}
$$

即 $R(s)$ 是当马尔可夫过程起始于状态 $s$ 时，第一次状态转移获得的奖励的期望值。正如前面假设的状态转移概率不变那样，我们将做出如下假设：

<span id="eq5">$\bullet$ 不变的奖励（stationary rewards）：马尔可夫奖励过程中的奖励是不变的意味着它们是时间无关的。在确定的奖励的情况下，数学上这表示对于过程的所有实现，一定有：</span>

$$
r_i=r_j, \quad \text{whenever}\text{ }s_i=s_j, \forall i,j=0,1,...，
\tag{5}
$$

<span id="eq6">在随机的奖励的情况下，我们要求以当前状态为条件的奖励的累积分布函数（cumulative distribution function，cdf）是时间无关的，数学上表示为：</span>

$$
F(r_i|s_i=s)=F(r_j|s_j=s), \quad \forall s\in S, \forall i,j=0,1,...，
\tag{6}
$$

<span id="eq7">这里 $F(r_i|s_i=s)$ 为以状态 $s_i=s$ 为条件时 $r_i$ 的累积分布函数。注意，根据式（[5](#eq5)）或式（[6](#eq6)），我们可以用下式来表示期望奖励：</span>

$$
R(s) = \mathbb{E}[r_i|s_i=s], \quad \forall i=0,1,...。
\tag{7}
$$

可以看到，只要马尔可夫奖励过程的“不变的奖励”的假设是真的，那么就只有期望奖励 $R$ 才是我们所关心的，并且我们可以不使用 $r_i$ 这个量。因此，以后“奖励”这个词将被交替使用来表示 $R$ 和 $r_i$，并且应该很容易通过上下文来理解。最后注意，在有限状态空间 $S$ 的情况下，$R$ 可以表示为一个维度为 $|S|$ 的向量。

**练习 3.4** （a）在假设状态转移概率和奖励不变的情况下，证明式（[7](#eq7)）。

#### 3.2.2 时间范围，回报和值函数（Horizon, Return and Value Function）

接下来我们定义马尔可夫奖励过程的时间范围、回报和值函数。

$\bullet$ 时间范围（Horizon）：马尔可夫奖励过程的时间范围 $H$ 定义为每个片段的时间步的数量。时间范围可以是有限的，也可以是无限的。如果时间范围是有限的，那么这个过程也称作有限马尔可夫奖励过程。

<span id="eq8">$\bullet$ 回报（Return）：马尔可夫奖励过程的回报 $G_t$ 定义为从时间 $t$ 开始，到时间范围 $H$ 为止所得到的衰减奖励和：</span>

$$
G_t = \sum_{i=t}^{H-1}\gamma^{i-t}r_i, \quad \forall 0\leq t\leq H-1。
\tag{8}
$$

<span id="eq9">$\bullet$ 状态值函数（State value function）：马尔可夫奖励过程的状态值函数 $V_t(s)$ 定义为状态 $s$ 在时间 $t$ 的期望回报：</span>

$$
V_t(s)=\mathbb{E}[G_t|s_t=s]。
\tag{9}
$$

注意，当 $H$ 是无限的时，通过式（[9](#eq9)）和奖励与状态转移概率不变的假设，我们可以得出：$V_i(s)=V_j(s)$，对于所有的 $i,j=0,1,...$，因此在这种情况下我们可以定义：

$$
V(s) = V_0(s)。
\tag{10}
$$

**练习 3.5** （a）如果奖励和状态转移概率不变的假设成立，且时间范围 $H$ 是无限的，使用式（[8](#eq8)）和式（[9](#eq9)）证明 $V_i(s)=V_j(s)$，对于所有的 $i,j=0,1,...$。

#### 3.2.3 衰减系数（Discount Factor）

注意，式（[8](#eq8)）定义的回报 $G_t$ 中，如果时间范围是无限的并且 $\gamma=1$，那么即使奖励都是有界的，回报也可能是无限的。如果这样，那么值函数 $V(s)$ 也是无限的，计算机无法解决这种问题。为了避免这种数学困难并使问题在计算上易于处理，我们设置 $\gamma<1$，这样在按照式（[8](#eq8)）计算回报时，未来奖励的贡献度会指数型下降。这里 $\gamma$ 被称为衰减系数（discount factor）。除了纯粹的计算原因之外，注意，人类的行为方式几乎是一样的：我们倾向于把即时奖励放在比后续奖励更重要的位置。对 $\gamma$ 的解释是，当 $\gamma=0$ 时，我们只关心即时奖励，当 $\gamma=1$ 时，我们视即时奖励和未来奖励同等重要。最后注意，如果马尔可夫奖励过程的时间范围是有限的，即 $H<\infty$，那么我们可以设置 $\gamma=1$，因为回报和值函数总是有限的。

**练习 3.6** 考虑有限时间范围和有限奖励的马尔可夫奖励过程。假设 $\exists M\in(0,\infty)$，使得对于所有的片段（实现）以及 $\forall i$，都有 $|r_i|\leq M$。（a）证明：对于所有片段，式（[8](#eq8)）定义的回报 $G_t$ 是有界的；（b）能否找到一个 $C(M,\gamma,t,H)$ 使得 对于所有片段，都有 $|G_t|\leq C$？

**练习 3.7** 考虑无限时间范围、有限奖励和 $\gamma<1$ 的马尔可夫奖励过程。（a）证明：对于所有片段，式（[8](#eq8)）定义的回报 $G_t$ 收敛到一个有限值。（提示：考虑 $S_n=\sum_{i=t}^{N}\gamma^{i-t}r_i$，这里 $N\geq t$，证明 $\{S_N\}_{N\geq t}$ 是一个柯西数列。）

#### 3.2.4 马尔可夫奖励过程示例：火星探测器（Example of a Markov Reward Process: Mars Rover）

考虑[图 2](#fig2) 所示的马尔可夫奖励过程，其状态和状态转移概率和**练习 3.3**中的火星探测器例子完全相同。在 ${S2,S3,S4,S5,S6}$ 中的任一状态执行动作得到的奖励都为 $0$，在 $S1$ 执行动作的奖励为 $1$，在 $S7$ 执行动作的奖励为 $10$。奖励是不变且确定的。这个例子中假设 $\gamma=0.5$。

**图2**

我们再次建设探测器的初始状态为 $S4$。考虑有限时间范围的情况 $H=4$。一些可能的片段以及回报 $G_0$ 如下所示：

$- S4,S5,S6,S7,S7$：$G_0 = 0 + 0.5\times 0 + 0.5^2\times 0 + 0.5^3\times 10=1.25$

$- S4,S4,S5,S4,S5$：$G_0 = 0 + 0.5\times 0 + 0.5^2\times 0 + 0.5^3\times 0=0$

$- S4,S3,S2,S1,S2$：$G_0 = 0 + 0.5\times 0 + 0.5^2\times 0 + 0.5^3\times 1=0.125$

### 3.3 计算马尔可夫奖励过程的值函数（Computing the Value Function of a Markov Reward Process）

这一部分我们介绍三种计算马尔可夫奖励过程的值函数的方法：

$\bullet$ 仿真（Simulation）

$\bullet$ 解析解（Analytic solution）

$\bullet$ 迭代解（Iterative solution）

#### 3.3.1 蒙特卡洛仿真（Monte Carlo Simulation）

第一种方法是使用马尔可夫奖励过程的转移概率模型和奖励来生成大量的片段，对于每个片段可以计算其回报，然后对其求平均值得到平均回报。对于一个马尔可夫奖励过程 $M=(S,\mathbf{P},R,\gamma)$，状态 $s$，时间 $t$，以及片段仿真次数 $N$，仿真算法的伪代码如算法 1 所示。

**算法1**

#### 3.3.2 解析解（Analytic Solution）

这种方法只适用于时间范围无限且 $\gamma<1$ 的马尔可夫奖励过程。通过式（[9](#eq9)）以及前面的假设，对于 $\forall s\in S$ 有

$$
V(s) \overset{(a)}{=} V_0(s) = \mathbb{E}[G_0|s_0=s] = \mathbb{E}[\sum_{i=0}^{\infty}\gamma^i r_i|s_0=s] = \mathbb{E}[r_0|s_0=s] + \sum_{i=1}^{\infty}\gamma^i \mathbb{E}[r_i|s_0=s]
$$